# Quality Gate Decision: Story 2.6 - FAQ Knowledge Base Responses

schema: 1
story: "2.6"
story_title: "FAQ Knowledge Base Responses"
gate: FAIL
status_reason: "Critical test failures and missing unit tests violate AC #9 and #10. Integration tests fail due to improper mocking strategy, and required unit tests for 10 FAQ variations per category are completely missing."
reviewer: "Quinn (Test Architect)"
updated: "2025-10-28T00:00:00Z"

waiver: { active: false }

top_issues:
  - id: "TEST-001"
    severity: high
    finding: "All 7 integration tests failing due to improper LLM mocking. Module-level instantiation of ChatAnthropic in faq.py:25 prevents test mocking from working correctly."
    suggested_action: "Refactor faq.py to accept LLM instance as parameter or use dependency injection pattern. Update tests to patch at correct location."
    refs: ["agent/nodes/faq.py:25", "tests/integration/test_faq_flow.py:97-101"]
    suggested_owner: "dev"

  - id: "TEST-002"
    severity: high
    finding: "AC #10 requires 'Unit test: 10 variations per FAQ → verify detection' but tests/unit/test_faq_detection.py does not exist. This is explicitly defined in Task 8 of the story."
    suggested_action: "Create tests/unit/test_faq_detection.py with 50 total test variations (10 per FAQ category) as specified in AC #10 and Task 8."
    refs: ["docs/stories/2.6.faq-knowledge-base-responses.md:217-241"]
    suggested_owner: "dev"

  - id: "ARCH-001"
    severity: medium
    finding: "Module-level LLM instantiation (line 25) violates testability best practices. Makes unit testing impossible without real API calls."
    suggested_action: "Use dependency injection or lazy initialization pattern to allow test mocking. Follow pattern: llm = get_llm() or pass llm as function parameter."
    refs: ["agent/nodes/faq.py:25"]
    suggested_owner: "dev"

  - id: "COV-001"
    severity: medium
    finding: "Code coverage at 28.41% is significantly below required 85% threshold. FAQ nodes at 36.76% coverage."
    suggested_action: "Add comprehensive unit tests for all FAQ detection branches and error paths to reach 85%+ coverage."
    refs: ["agent/nodes/faq.py"]
    suggested_owner: "dev"

risk_summary:
  totals: { critical: 0, high: 2, medium: 2, low: 0 }
  highest:
    level: high
    probability: certain
    impact: critical
    description: "Test failures block story completion and violate explicit acceptance criteria"
  recommendations:
    must_fix:
      - "Fix integration test mocking to allow tests to pass"
      - "Create missing unit test file with 50 FAQ variation tests"
    monitor:
      - "Track code coverage improvement to 85% target"
      - "Monitor test execution time as LLM mocking is added"

# Quality Score: 100 - (20 × 2 high) - (10 × 2 medium) = 40/100
quality_score: 40

evidence:
  tests_reviewed: 7
  tests_passing: 0
  tests_failing: 7
  risks_identified: 4
  trace:
    ac_covered: [1, 2, 3, 4, 5, 6, 7, 8]  # Implementation present for these ACs
    ac_gaps: [9, 10]  # AC 9 (integration tests) and AC 10 (unit tests) not satisfied

nfr_validation:
  security:
    status: PASS
    notes: "No sensitive data handling in FAQ flow. Database queries use parameterized statements. No authentication/authorization concerns for FAQ retrieval."

  performance:
    status: CONCERNS
    notes: "Claude API call adds ~1-2s latency per FAQ detection. No caching implemented. FAQ answers retrieved from DB on every request (acceptable given indexed lookup)."

  reliability:
    status: PASS
    notes: "Comprehensive error handling with graceful fallbacks. Logging includes structured fields for traceability. FAQ not found returns helpful fallback message."

  maintainability:
    status: CONCERNS
    notes: "Module-level LLM instantiation hurts testability. Otherwise good: clear separation of concerns, helper functions used correctly, immutable state pattern followed."

recommendations:
  immediate:  # Must fix before Ready for Done
    - action: "Refactor LLM instantiation to support dependency injection for testing"
      refs: ["agent/nodes/faq.py:25"]
      estimated_effort: "30 minutes"

    - action: "Fix integration test mocking strategy (patch llm instance at correct location)"
      refs: ["tests/integration/test_faq_flow.py:97-147"]
      estimated_effort: "1 hour"

    - action: "Create tests/unit/test_faq_detection.py with 50 FAQ variation tests per AC #10"
      refs: ["docs/stories/2.6.faq-knowledge-base-responses.md:217-241"]
      estimated_effort: "2 hours"

    - action: "Verify all tests pass before marking story as Done"
      refs: []
      estimated_effort: "30 minutes"

  future:  # Can be addressed in follow-up stories
    - action: "Consider caching FAQ detection results for repeated questions within same conversation"
      refs: ["agent/nodes/faq.py:77"]
      estimated_effort: "2 hours"

    - action: "Add performance test to verify FAQ response time < 3 seconds end-to-end"
      refs: []
      estimated_effort: "1 hour"

    - action: "Consider pre-loading FAQ policies at service startup to avoid DB query on every answer"
      refs: ["agent/nodes/faq.py:152-156"]
      estimated_effort: "1 hour"

# Detailed requirements traceability
requirements_trace:
  AC_1:
    requirement: "FAQ data stored in `policies` table with JSON structure"
    implementation: "database/seeds/faqs.py:16-109 - JSONB structure with required fields"
    test_coverage: "Integration tests verify retrieval, but missing unit tests"
    status: PASS

  AC_2:
    requirement: "Seed script populates minimum 5 FAQs"
    implementation: "database/seeds/faqs.py:16-109 - All 5 FAQs defined"
    test_coverage: "Manual verification required (seed script not tested)"
    status: PASS

  AC_3:
    requirement: "`detect_faq_intent` node uses Claude for classification"
    implementation: "agent/nodes/faq.py:28-118 - Claude Sonnet 4 used"
    test_coverage: "Integration tests fail due to mocking issues"
    status: CONCERNS

  AC_4:
    requirement: "If FAQ detected → `answer_faq` retrieves from database"
    implementation: "agent/nodes/faq.py:121-225 - DB query with JSONB extraction"
    test_coverage: "Integration tests fail, unit tests missing"
    status: CONCERNS

  AC_5:
    requirement: "Answers formatted with Maite's tone + emojis"
    implementation: "database/seeds/faqs.py answers include emojis and warm tone"
    test_coverage: "Integration tests verify emoji presence (when fixed)"
    status: PASS

  AC_6:
    requirement: "After FAQ → proactive follow-up question"
    implementation: "agent/nodes/faq.py:185-186 - Appends follow-up text"
    test_coverage: "Integration tests verify (when fixed)"
    status: PASS

  AC_7:
    requirement: "Sample FAQs implemented per AC"
    implementation: "database/seeds/faqs.py:16-109 - All 5 FAQs match spec"
    test_coverage: "Manual verification"
    status: PASS

  AC_8:
    requirement: "If location FAQ → optionally offer Google Maps link"
    implementation: "agent/nodes/faq.py:181-183 - Conditional link addition"
    test_coverage: "Integration test_faq_flow_location_with_maps_link (fails)"
    status: CONCERNS

  AC_9:
    requirement: "Integration test: '¿Abrís los sábados?' → verify answer → verify follow-up"
    implementation: "tests/integration/test_faq_flow.py:69-119"
    test_coverage: "TEST FAILS - mocking issue"
    status: FAIL

  AC_10:
    requirement: "Unit test: 10 variations per FAQ → verify detection"
    implementation: "MISSING - tests/unit/test_faq_detection.py does not exist"
    test_coverage: "0% - file not created"
    status: FAIL

# Test execution summary
test_results:
  integration_tests:
    total: 7
    passed: 0
    failed: 7
    skipped: 0
    failure_details:
      - test: "test_faq_flow_hours"
        reason: "ChatAnthropic mocking doesn't work due to module-level instantiation"
      - test: "test_faq_flow_parking"
        reason: "AttributeError: 'ChatAnthropic' object has no attribute 'ainvoke'"
      - test: "test_faq_flow_location_with_maps_link"
        reason: "Mocking issue"
      - test: "test_faq_flow_cancellation_policy"
        reason: "Mocking issue"
      - test: "test_faq_flow_payment_info"
        reason: "Mocking issue"
      - test: "test_non_faq_message_routes_to_booking_flow"
        reason: "Mocking issue"
      - test: "test_faq_multiple_variations_same_category"
        reason: "Mocking issue"

  unit_tests:
    total: 0
    passed: 0
    failed: 0
    skipped: 0
    note: "tests/unit/test_faq_detection.py does not exist - AC #10 violation"

# Code quality observations
code_quality:
  positive:
    - "Comprehensive error handling with try-except blocks in both nodes"
    - "Structured logging with conversation_id, customer_id, faq_id for traceability"
    - "Immutable state pattern followed correctly using add_message helper"
    - "Clear separation of concerns between detection and answering"
    - "Fallback messages provide good user experience on errors"
    - "JSONB structure allows flexible FAQ data without schema migrations"

  concerns:
    - "Module-level LLM instantiation prevents proper unit testing"
    - "No input validation on detected_faq_id before database query"
    - "Google Maps URL hardcoded (should be in config or environment variable)"
    - "No retry logic for Claude API failures"
    - "FAQ seed script has no tests to verify data integrity"

# Architecture alignment
architecture_review:
  langgraph_integration: PASS
  database_design: PASS
  state_management: PASS
  testability: FAIL
  observability: PASS
  error_handling: PASS

# Final recommendation
recommendation:
  status: "Changes Required"
  blocking_issues: 2
  rationale: |
    Story cannot be marked as Done due to:
    1. All 7 integration tests failing (AC #9 violation)
    2. Required unit tests completely missing (AC #10 violation)

    Implementation quality is good overall, but testing deficiencies are critical blockers.
    Estimated 3-4 hours of work to fix mocking, create unit tests, and verify all tests pass.

  next_steps:
    - "Dev: Refactor LLM instantiation for testability"
    - "Dev: Fix integration test mocking"
    - "Dev: Create unit test file with 50 variations"
    - "Dev: Run full test suite and verify 85%+ coverage"
    - "QA: Re-review after fixes"
